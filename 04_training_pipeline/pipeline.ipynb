{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build an CV Training Pipeline using SageMaker Pipeline\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [Dataset](#Dataset)\n",
    "5. [Build SageMaker Pipeline](#Build-SageMaker-Pipeline)\n",
    "    1. [Bring Your Own Container (BYOC)](#Bring-Your-Own-Container-(BYOC))\n",
    "    2. [Set Pipeline input parameters](#Set-Pipeline-input-parameters)\n",
    "    3. [Define Cache Configuration](#Define-Cache-Configuration)\n",
    "    4. [Preprocess data step](#Preprocess-data-step)\n",
    "    5. [Training step](#Training-step)\n",
    "    6. [Model evaluation step](#Model-evaluation-step)\n",
    "    7. [Register model step](#Register-model-step)\n",
    "    8. [Accuracy condition step](#Accuracy-condition-step)\n",
    "    9. [Pipeline Creation](#Pipeline-Creation)\n",
    "    10. [Submit and trig pipeline](#Submit-and-trig-pipeline)\n",
    "    11. [Analyzing Results](#Analyzing-Results)\n",
    "6. [Execute same pipeline in one continuous script](#Execute-same-pipeline-in-one-continuous-script)\n",
    "7. [Build Custom Project Templates (Optional)](#Build-Custom-Project-Templates-(Optional))\n",
    "    1. [Setup Service Catalog Portfolio](#Setup-Service-Catalog-Portfolio)\n",
    "7. [Clean Up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrate how to build a reusable computer vision (CV) pattern using **SageMaker Pipeline**. This particular pattern goes through preprocessing, training, and evaluating steps for 2 different training jobs:1) Spot training and 2) On Demand training.  If the accuracy meets certain requirements, the models are then registered with SageMaker Model Registry.\n",
    "\n",
    "We have also tagged the training workloads: `TrainingType: Spot or OnDemand`.  If you are interested and have permission to access billing of your AWS account, you the can see the cost savings from spot training from the side-by-side comparison. To enable custom cost allocation tags, please follow this [AWS documentation](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html).  It takes 12-48 hrs for the new tag to show in your cost explore.\n",
    "\n",
    "![Spot Training](statics/cost-explore.png)\n",
    "\n",
    "SageMaker pipelines works on the concept of steps. The order steps are executed in is inferred from the dependencies each step have. If a step has a dependency on the output from a previous step, it's not executed until after that step has completed successfully. This also allows SageMaker to create a **Direct Acyclic Graph, DAG,** that can be visuallized in Amazon SageMaker Studio (see diagram below). The DAG can be used to track pipeline executions, inputs/outputs and metrics, giving user the full lineage of the model creation.\n",
    "\n",
    "![Training Pipeline](statics/cv-training-pipeline.png)\n",
    "\n",
    "** Note: This Notebook was tested on Data Science Kernel in SageMaker Studio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you can simply execute each cell in order. To understand what's happening, you'll need:\n",
    "\n",
    "- Access to the SageMaker default S3 bucket\n",
    "- Access to Elastic Container Registry (ECR)\n",
    "- For the optional portion of this lab, you will need access to CloudFormation, Service Catalog, and Cost Explorer\n",
    "- Familiarity with Training on Amazon SageMaker\n",
    "- Familiarity with Python\n",
    "- Familiarity with AWS S3\n",
    "- Basic understanding of CloudFormaton and concept of deploy infra as code\n",
    "- Basic understanding of tagging and cost governance\n",
    "- Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\n",
    "- SageMaker Studio is preferred for the full UI integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define the sagemaker session, default bucket, job prefixes, pipeline and model group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker --quiet # Ensure latest version of SageMaker is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account = sagemaker_session.account_id()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\n",
    "prefix = \"cv-sagemaker-immersionday\" # or define your own prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = f\"{prefix}-model-group\"  # Model name in model registry\n",
    "pipeline_name = f\"{prefix}-pipeline\"  # SageMaker Pipeline name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we are using is from [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html). \n",
    "\n",
    "Here we are using the artifact from previous labs:\n",
    "\n",
    "- S3 path to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_data = f's3://{bucket}/{prefix}/full/data'\n",
    "print(s3_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture the ECR URI here from [model_evaluation lab](../03_model_evaluation/model-evaluation-processing-job.ipynb), we may use it later on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"sagemaker-tf-container\"\n",
    "container_version = \"2.0\"\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:{}\".format(account, region, container_name, container_version)\n",
    "\n",
    "print(f'image_uri: {image_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Pipeline input parameters\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "* ParameterInteger - represents an int Python type\n",
    "* ParameterFloat - represents a float Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "![Parameter Input](statics/parameters-input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and model registration, as depicted in picture below.\n",
    "    \n",
    "![Pipeline](statics/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# Parameters for pipeline execution\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\", default_value=1\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=s3_raw_data\n",
    ")\n",
    "\n",
    "input_annotation = ParameterString(\n",
    "    name=\"AnnotationFileName\",\n",
    "    default_value=\"classes.txt\"\n",
    ")\n",
    "\n",
    "# This is a large dataset, we are only going to train a subset of the classes\n",
    "class_selection = ParameterString(\n",
    "    name=\"ClassSelection\",\n",
    "    default_value=\"13, 17, 35, 36, 47, 68, 73, 87\" #If use the mini dataset, please make sure to use the class index with the available list\n",
    ")\n",
    "\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_count = 1\n",
    "training_instance_type = \"ml.c5.4xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cache Configuration\n",
    "When step cache is defined, before SageMaker Pipelines executes a step, it attempts to find a previous execution of a step that was called with the same arguments.\n",
    "\n",
    "Pipelines doesn't check whether the actual data or code that the arguments point to has changed. If a previous execution is found, Pipelines will propagates the values from the cache hit during execution, rather than recomputing the step.\n",
    "\n",
    "Step caching is available for the following step types:\n",
    "\n",
    "* Training\n",
    "* Tuning\n",
    "* Processing\n",
    "* Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "## By enabling cache, if you run this pipeline again, without changing the input \n",
    "## parameters it will skip the training part and reuse the previous trained model\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data step\n",
    "We are taking the original code in Jupyter notebook and create a containerized script to run in a preprocessing job.\n",
    "\n",
    "The [preprocess.py](./preprocess.py) script takes in the raw images files and splits them into training, validation and test sets by class.\n",
    "It merges the class annotation files so that you have a manifest file for each separate data set. And exposes two parameters: classes (allows you to filter the number of classes you want to train the model on; default is all classes) and input-data (the human readable name of the classes).\n",
    "\n",
    "We are going to use **SKLearnProcessor** to process the data. For more detail on different type of processing jobs, please refer to the amazon documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html).\n",
    "\n",
    "![Pipeline](statics/pipeline-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "# SKlearnProcessor for preprocessing\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(base_job_name = f\"{prefix}-preprocess\",  # choose any name\n",
    "                                    framework_version='0.20.0',\n",
    "                                    role=role,\n",
    "                                    instance_type=processing_instance_type,\n",
    "                                    instance_count=processing_instance_count)\n",
    "\n",
    "output_s3_uri = f's3://{bucket}/{prefix}/outputs/pipelines/{uuid.uuid4()}'\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"BirdClassificationPreProcess\",  # choose any name\n",
    "    processor=sklearn_processor,\n",
    "    code=\"preprocess.py\",\n",
    "    job_arguments=[\"--classes\", class_selection,\n",
    "                \"--input-data\", input_annotation],\n",
    "    inputs=[ProcessingInput(source=input_data, \n",
    "            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train_data', \n",
    "                         source=\"/opt/ml/processing/output/train\", \n",
    "                         destination = output_s3_uri +'/train'),\n",
    "        ProcessingOutput(output_name='val_data',\n",
    "                         source=\"/opt/ml/processing/output/validation\", \n",
    "                         destination = output_s3_uri +'/validation'),\n",
    "        ProcessingOutput(output_name='test_data',\n",
    "                         source=\"/opt/ml/processing/output/test\", \n",
    "                         destination = output_s3_uri +'/test'),\n",
    "        ProcessingOutput(output_name='manifest',\n",
    "                         source=\"/opt/ml/processing/output/manifest\", \n",
    "                         destination = output_s3_uri +'/manifest'),\n",
    "    ],\n",
    "    cache_config=cache_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "We are using SageMaker's TensorFlow container, the custom TensorFlow training code is provided via a Python script in a separate file that gets passed to SageMaker ([train-mobilenet.py](./code/train-mobilenet.py)).\n",
    "\n",
    "Our Pipeline experiments with 2 training jobs, Spot and On Demand, side-by-side.  Each workload is tagged using 'TrainingType'.  It you have the permission, you can enable the User defined tag in Cost Explore and compare the cost difference between spot and on demand training.  [Here](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html) is how to enable user-defined tags.\n",
    "\n",
    "![Pipeline](statics/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "TF_FRAMEWORK_VERSION = '2.4.1'\n",
    "\n",
    "hyperparameters = {'initial_epochs':     5,\n",
    "                   'batch_size':         8,\n",
    "                   'fine_tuning_epochs': 20, \n",
    "                   'dropout':            0.4,\n",
    "                   'data_dir':           '/opt/ml/input/data'}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',      'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'acc',       'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'val_loss',  'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'val_acc',   'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]\n",
    "\n",
    "if training_instance_count > 1:\n",
    "    distribution = {'parameter_server': {'enabled': True}}\n",
    "    DISTRIBUTION_MODE = 'ShardedByS3Key'\n",
    "else:\n",
    "    distribution = {'parameter_server': {'enabled': False}}\n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "train_in = TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "                         distribution=DISTRIBUTION_MODE)\n",
    "test_in  = TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri,\n",
    "                         distribution=DISTRIBUTION_MODE)\n",
    "val_in   = TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"val_data\"].S3Output.S3Uri,\n",
    "                         distribution=DISTRIBUTION_MODE)\n",
    "\n",
    "inputs = {'train':train_in, 'test': test_in, 'validation': val_in}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = dict()\n",
    "training_estimators = dict()\n",
    "models = dict()\n",
    "\n",
    "training_options = ['Spot', 'OnDemand']\n",
    "\n",
    "for t in training_options:\n",
    "    tags = dict()\n",
    "    tags['Key'] = 'TrainingType'\n",
    "    tags['Value'] = t\n",
    "        # Training step for generating model artifacts\n",
    "    model_path = f\"{output_s3_uri}/models\"\n",
    "    checkpoint_s3_uri = f\"{output_s3_uri}/outputcheckpoints\"\n",
    "    \n",
    "    if t.lower() == 'spot':\n",
    "        estimator = TensorFlow(entry_point='train-mobilenet.py',\n",
    "                               source_dir='code',\n",
    "                               output_path=model_path,\n",
    "                               instance_type=training_instance_type,\n",
    "                               instance_count=training_instance_count,\n",
    "                               distribution=distribution,\n",
    "                               hyperparameters=hyperparameters,\n",
    "                               metric_definitions=metric_definitions,\n",
    "                               role=role,\n",
    "                               use_spot_instances=True,\n",
    "                               max_run=60*60*10,\n",
    "                               max_wait=60*60*12, # Seconds to wait for spot instances to become available\n",
    "                               checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                               framework_version=TF_FRAMEWORK_VERSION, \n",
    "                               py_version='py37',\n",
    "                               base_job_name=prefix,\n",
    "                               script_mode=True,\n",
    "                               tags=[tags])\n",
    "    else:\n",
    "        estimator = TensorFlow(entry_point='train-mobilenet.py',\n",
    "                       source_dir='code',\n",
    "                       output_path=model_path,\n",
    "                       instance_type=training_instance_type,\n",
    "                       instance_count=training_instance_count,\n",
    "                       distribution=distribution,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=role,\n",
    "                       framework_version=TF_FRAMEWORK_VERSION, \n",
    "                       py_version='py37',\n",
    "                       base_job_name=prefix,\n",
    "                       script_mode=True,\n",
    "                       tags=[tags])\n",
    "        \n",
    "    step_train = TrainingStep(\n",
    "        name=f\"BirdClassification{t}Train\",\n",
    "        estimator=estimator,\n",
    "        inputs=inputs,\n",
    "        cache_config=cache_config\n",
    "    )\n",
    "    \n",
    "    training_steps[t] = step_train\n",
    "    training_estimators[t] = estimator\n",
    "    models[t] = step_train.properties.ModelArtifacts.S3ModelArtifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation step\n",
    "We are going to use a ProcessingStep for our model evaluation, and we are going to use our own container from the earlier step.\n",
    "\n",
    "[evaluation.py](./evaluation.py) script  does the following:\n",
    "1. Load the tf model \n",
    "2. Run prediction\n",
    "3. Compare predicts vs actuals and generate the confussion matrix\n",
    "\n",
    "![Pipeline](statics/pipeline-3.png)\n",
    "\n",
    "\n",
    "When you register this model to the model registery, metrics generated from this step will be attached to the model version and can be visualized in SageMaker Studio like this:\n",
    "\n",
    "![Confusion Matrix](statics/confussion_matrix.png)\n",
    "\n",
    "Here are more details on the list of metric available for each type of ML problems: [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "\n",
    "eval_steps = dict()\n",
    "eval_reports = dict()\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "for t in training_options:\n",
    "    \n",
    "    eval_processor = ScriptProcessor(\n",
    "        base_job_name = f\"{prefix}-evaluation\",\n",
    "        command=['python3'],\n",
    "        image_uri=image_uri,\n",
    "        role=role,\n",
    "        instance_count=processing_instance_count,\n",
    "        instance_type=processing_instance_type,\n",
    "        sagemaker_session = pipeline_session)\n",
    "        \n",
    "    evaluation_report = PropertyFile(\n",
    "        name=f\"Evaluation{t}Report\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    \n",
    "    step_eval = ProcessingStep(\n",
    "        name=f\"BirdClassification{t}Eval\",\n",
    "        processor=eval_processor,\n",
    "        cache_config=cache_config,\n",
    "        inputs=[\n",
    "            ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri, \n",
    "                            destination=\"/opt/ml/processing/input/test\"),\n",
    "            ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[\"manifest\"].S3Output.S3Uri, \n",
    "                            destination=\"/opt/ml/processing/input/manifest\"),\n",
    "            ProcessingInput(source=models[t], \n",
    "                            destination=\"/opt/ml/processing/model\"),\n",
    "           ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")\n",
    "        ],\n",
    "        code=\"evaluation.py\",\n",
    "        property_files=[evaluation_report],\n",
    "    )\n",
    "    \n",
    "    eval_steps[t] = step_eval\n",
    "    eval_reports[t] = evaluation_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model step\n",
    "In this step, the resulting model artifacts is register as a model package in a model package group. \n",
    "\n",
    "A model package is a reusable model artifacts abstraction that packages all ingredients required for inference. It also captures the metrics from the evaluation step for future comparison.\n",
    "\n",
    "A model package group is a collection of model packages, usually different model versions.  It also enables the user to compare metric accross different models.  \n",
    "\n",
    "Specifically, pass in the S3ModelArtifacts from the TrainingStep, step_train properties. The TrainingStep properties attribute matches the object model of the DescribeTrainingJob response object.\n",
    "\n",
    "![Pipeline](statics/pipeline-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "model_register_steps = dict()\n",
    "\n",
    "for t in training_options:\n",
    "    # Create ModelMetrics object using the evaluation report from the evaluation step\n",
    "    # A ModelMetrics object contains metrics captured from a model.\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                eval_steps[t].arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "                    \"S3Uri\"\n",
    "                ]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Crete a RegisterModel step, which registers the model with Sagemaker Model Registry.\n",
    "    step_register = RegisterModel(\n",
    "        name=f\"Register{t}Model\",\n",
    "        estimator=training_estimators[t],\n",
    "        model_data=models[t],\n",
    "        content_types=[\"application/x-image\"],\n",
    "        response_types=[\"application/json\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        model_metrics=model_metrics,\n",
    "    )\n",
    "    \n",
    "    model_register_steps[t] = step_register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy condition step\n",
    "This condition step only allows the model to be registered if the accuracy of the model, as determined by the evaluation step step_eval, exceeded a specified value. A ConditionStep enables pipelines to support conditional execution in the pipeline DAG based on the conditions of the step properties.\n",
    "\n",
    "![Pipeline](statics/pipeline-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "condition_steps = dict()\n",
    "\n",
    "for t in training_options:\n",
    "    \n",
    "    # Create accuracy condition to ensure the model meets performance requirements.\n",
    "    # Models with a test accuracy lower than the condition will not be registered with the model registry.\n",
    "    cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "        left=JsonGet(\n",
    "            step_name=eval_steps[t].name,\n",
    "            property_file=eval_reports[t],\n",
    "            json_path=\"multiclass_classification_metrics.accuracy.value\",\n",
    "        ),\n",
    "        right=0.7,\n",
    "    )\n",
    "\n",
    "    # Create a Sagemaker Pipelines ConditionStep, using the condition above.\n",
    "    # Enter the steps to perform if the condition returns True / False.\n",
    "    step_cond = ConditionStep(\n",
    "        name=f\"BirdClassification{t}Condition\",\n",
    "        conditions=[cond_gte],\n",
    "        if_steps=[model_register_steps[t]],\n",
    "        else_steps=[],\n",
    "    )\n",
    "    \n",
    "    condition_steps[t] = step_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Creation\n",
    "\n",
    "Last step is to combine all the previous steps into a Pipeline so it can be executed.\n",
    "\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# Each parameter for the pipeline must be set as a parameter explicitly when the pipeline is created.\n",
    "\n",
    "# build the steps\n",
    "steps = [step_process]\n",
    "for t in training_steps:\n",
    "    steps.append(training_steps[t])\n",
    "    \n",
    "for e in eval_steps:\n",
    "    steps.append(eval_steps[e])\n",
    "    \n",
    "for c in condition_steps:\n",
    "    steps.append(condition_steps[c])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        input_data,\n",
    "        input_annotation,\n",
    "        class_selection\n",
    "    ],\n",
    "    steps=steps,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and trig pipeline\n",
    "Submit the pipeline definition to the Pipeline service. The role passed in will be used by the Pipeline service to create all the jobs defined in the steps.\n",
    "\n",
    "Once a pipeline has been submited (pipeline.upsert()), user can trigger the pipeline using the API (pipeline.start()) or through the SageMaker Studo UI:\n",
    "\n",
    "![Pipeline UI Trigger](statics/studio-ui-pipeline.png)\n",
    "\n",
    "![Pipeline Code Trigger](statics/execute-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit pipline\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Execute pipeline using the default parameters.\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Results\n",
    "You can compre different version of model by selecting multiple versions and right-click -> Compare model versions.  If you have visuallizations, you graph may overlap depending on how complete your use case is.\n",
    "\n",
    "![Model Comparison](statics/compare-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approval for deployment\n",
    "\n",
    "---\n",
    "After you create a model version, you typically want to evaluate its performance before you deploy it. So the pipeline default the approval status to `PendingManualApproval`. You can manually update or update using API to change the status to Approved or Rejected.  Here is how you manually update from SageMaker studio UI:\n",
    "\n",
    "![Manual Approval](statics/manual_approval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon EventBridge monitors status change events in Amazon SageMaker. EventBridge enables you to automate SageMaker and respond automatically to events such as a training job status change, endpoint status change, or **Model package state change**.\n",
    "\n",
    "Please reference [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-eventbridge.html#eventbridge-model-package) documentation for the entire event of Model package state change.\n",
    "\n",
    "To automate the deployment process, You can use event bridge to Invoke a **deployment Lambda function** that checks the `ModelApprovalStatus` attribute in the event. If the status is **Approved** the Lambda will continue with the deployement."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
