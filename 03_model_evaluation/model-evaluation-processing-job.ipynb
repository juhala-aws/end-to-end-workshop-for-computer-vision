{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation using SageMaker Processing Job\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [Dataset](#Dataset)\n",
    "5. [Build a SageMaker Processing Job](#Build-a-SageMaker-Processing-Job)\n",
    "    1. [Prepare the Script and Docker File](#Prepare-the-Script-and-Docker-File)\n",
    "    2. [Configure a ScriptProcessor](#Configure-a-ScriptProcessor)\n",
    "6. [Review Outputs](#Review-Outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Postprocess and Model evaluation is an important step to vet our models before deployment. \n",
    "\n",
    "In this lab you will use ScriptProcessor from SageMaker Process to build a post processing step after model training to evaluate the performance of the model.  \n",
    "\n",
    "To setup your ScriptProcessor, we will build a custom container for a model evaluation script which will Load the tensorflow model, Load the test dataset and annotation (from previous module), and then run prediction and generate the confusion matrix. \n",
    "\n",
    "** Note: This Notebook was tested on Data Science Kernel in SageMaker Studio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Download the notebook into your environment, and you can run it by simply execute each cell in order. To understand what's happening, you'll need:\n",
    "\n",
    "- Access to the SageMaker default S3 bucket.\n",
    "- Familiarity with Python and numpy\n",
    "- Basic familiarity with AWS S3.\n",
    "- Basic understanding of AWS Sagemaker.\n",
    "- Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\n",
    "- SageMaker Studio is preferred for the full UI integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Setting up the environment, load the libraries, and define the parameter for the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account = sagemaker_session.account_id()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\n",
    "prefix = \"cv-sagemaker-immersionday\" # or define your own prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset we are using is from [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html). \n",
    "\n",
    "Here we are using the artifacts from previous labs:\n",
    "\n",
    "- S3 path for test image data\n",
    "- S3 path for test data annotation file\n",
    "- S3 path for the bird classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_images = f's3://{bucket}/{prefix}/outputs/test/'\n",
    "s3_manifest = f's3://{bucket}/{prefix}/outputs/manifest'\n",
    "s3_model = f's3://{bucket}/{prefix}/outputs/model/'\n",
    "\n",
    "print(f's3_images: {s3_images},\\ns3_manifest: {s3_manifest},\\ns3_model: {s3_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Script and Docker File\r\n",
    "With SageMaker, you can run data processing jobs using the SKLearnProcessor, popular ML frameworks processors, Apache Spark, or BYOC.  To learn more about visit [SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html).\r\n",
    "\r\n",
    "For this example we are going to practice using ScriptProcessor and Bring Your Own Container (BYOC). ScriptProcessor require you to feed a container URI from ECR and a custom script for the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the script\n",
    "\n",
    "Please inspect the [evaluation.py](evaluation.py) script that has been provided for you.\n",
    "\n",
    "Here is what the script [evaluation.py](evaluation.py) does:\n",
    "1. loading the tf model\n",
    "2. looping through the annotation file to run inference predictions\n",
    "3. tally the results using sklearn libraries & generate the confusion matrix\n",
    "4. save the metrics in a evaluation.json report as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring Your Own Container (BYOC)\n",
    "Below we build a custom docker container and push to Amazon Elastic Container Registry (ECR).\n",
    "\n",
    "You can use the standard TFflow container, but ScriptProcessor currently does not support `source_dir` for custom requirement.txt and multiple python file.  That is on the roadmap, please follow this [thread](https://github.com/aws/sagemaker-python-sdk/issues/1248) for updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/requirements.txt\n",
    "# This is the set of Python packages that will get pip installed\n",
    "# at startup of the Amazon SageMaker endpoint or batch transformation. \n",
    "Pillow\n",
    "scikit-learn\n",
    "pandas\n",
    "numpy\n",
    "tensorflow==2.10\n",
    "boto3==1.18.4\n",
    "sagemaker-experiments\n",
    "matplotlib==3.4.2\n",
    "seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM public.ecr.aws/docker/library/python:3.7\n",
    "    \n",
    "ADD requirements.txt /\n",
    "\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE \n",
    "ENV TF_CPP_MIN_LOG_LEVEL=\"2\"\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to build a container image and push to ECR is to use studio image builder. This require certain permission for your sagemaker execution role, which is already provided in this setup. \n",
    "\n",
    "But please check this [blog](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/) for additional information on how to use the Amazon SageMaker studio image build cli to build container images from your studio notebooks in case you need to update your role policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"sagemaker-tf-container\"\n",
    "container_version = \"2.0\"\n",
    "!cd docker && sm-docker build . --file Dockerfile --repository $container_name:$container_version\n",
    "    \n",
    "ecr_image = \"{}.dkr.ecr.{}.amazonaws.com/{}:{}\".format(account, region, container_name, container_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a ScriptProcessor\r\n",
    "\r\n",
    "1) copy the ECR uri from the step above\r\n",
    "\r\n",
    "2) initialize the Process (instance count, instance type, etc.)\r\n",
    "\r\n",
    "3) run the processing job (define script path, input arguments, input and output file locations\r\n",
    "\r\n",
    "Note: we are not using GPU, so you can ignore the CUDA warning message. You can add the corresponding libraries to you docker file if you want use GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput, Processor\n",
    "from sagemaker import get_execution_role\n",
    "import uuid\n",
    "\n",
    "image_uri = ecr_image\n",
    "\n",
    "s3_evaluation_output = f's3://{bucket}/{prefix}/outputs/evaluation'\n",
    "\n",
    "\n",
    "script_processor = ScriptProcessor(base_job_name = prefix,\n",
    "                command=['python3'],\n",
    "                image_uri=image_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(\n",
    "    code='evaluation.py',\n",
    "    arguments=[\"--model-file\", \"model.tar.gz\"],\n",
    "    inputs=[ProcessingInput(source=s3_images, \n",
    "                            destination=\"/opt/ml/processing/input/test\"),\n",
    "            ProcessingInput(source=s3_manifest, \n",
    "                            destination=\"/opt/ml/processing/input/manifest\"),\n",
    "            ProcessingInput(source=s3_model, \n",
    "                            destination=\"/opt/ml/processing/model\"),\n",
    "           ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\", \n",
    "                         destination=s3_evaluation_output),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Outputs\n",
    "\n",
    "At the end of the lab, you will generate a json file containing the performance metrics (accuracy, precision, recall, f1, and confusion matrix) on your test dataset.  Run the cell below to review the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "eval_matrix_key = f'{prefix}/outputs/evaluation/evaluation.json'\n",
    "content_object = s3.Object(bucket, eval_matrix_key)\n",
    "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file_content)\n",
    "\n",
    "pp.pprint(json_content['multiclass_classification_metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the confusion matrix output by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix_file = f's3://{bucket}/{prefix}/outputs/evaluation/confusion_matrix.png'\r\n",
    "!aws s3 cp $cf_matrix_file .\r\n",
    "\r\n",
    "from IPython import display\r\n",
    "display.Image(\"confusion_matrix.png\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
